---
title: "Zadanie 2 - PWC"
author: "Aleksander Mackiewicz-Kubiak"
date: "2024-11-29"
output:
  pdf_document: default
  html_document: default
---


## Pakiety

```{r message=FALSE, warning=FALSE}
library(tidyr)
library(gamlss)
library(dplyr)
library(fitdistrplus)
library(usefun)
library(quantmod)
library(scales)
library(copula)
library(psych)
options(scipen = 999)
```

## PKT 1

Dane, które dobieram do poprzednio wybranego indeksu S&P 500 to indeks giełdowy DJIA, czyli indeks notujący wartości z tych samych giełd co S&P 500 i który   notuje wartości dla jednych z największych amerykańskich przedsiębiorstw. Firmy te w większości również wliczają sie do indeksu S&P 500, stąd też zakładam, że notowania DJIA będą bezpośrednio powiązane z S&P 500, zwłaszcza jeśli dla obu wezme ceny zamknięcia w tym samym okresie. Wpierw sprawdzam ogólne charakterystyki danych:

```{r include=FALSE}
getSymbols(c("^GSPC", "^DJI"), src = "yahoo", from = "2021-11-01", to = "2024-10-30")
```

```{r echo=FALSE}
sp500_close <- Cl(GSPC)
DJIA <- Cl(DJI)
summary(sp500_close); summary(DJIA)
```

A następnie wyświetlam wykresy tych indeksów:

```{r echo=FALSE}
par(mfrow = c(2, 1))
plot(sp500_close, main="Ceny zamknięcia S&P 500")
plot(DJIA, main="Ceny zamknięcia DJIA")
```

Oba wykresy mają bardzo podobny kształt do siebie, jedyne różnią się skalą wartości na osi Y. Jedyna drobna różnica, którą mogę zauważyc to lekko inne wartości w okresie tuż po 1 listopada 2022 (stąd też potwierdzam, że te dane nie są identyczne). Zatem założenie o ich powiązaniu, patrząc tylkona te wykresy, jest poprawne.
## PKT 2 i 3

Drugim krokiem będzie wyznaczenie szeregów czasowych strat dla obu tych indeksów. W tym celu wpierw różnicuje osobno oba szeregi, by otrzymać dzienne zwroty, następnie ponieważ chce mieć same wartości strat, czyli wartości poniżej zera to usuwam wszystkie zerowe i dodatnie wartości z szeregów. Na koniec by móc dopasowywać szereg do rozkładu lognormlanego, który przyjmuje tylko wartości dodatnie, zmieniam znak wszystkich pozostałych wartości, i w ten sposób otrzymuje dodatni szereg czasowy strat.  

```{r echo=FALSE}
differ1<-diff(sp500_close)[-1]
plot(differ1, main="Zróżnicowane ceny S&P 500")
```

```{r echo=FALSE}
differ1 <- differ1[differ1 < 0]
plot(differ1, main="Straty S&P 500")
```


```{r echo=FALSE}
differ1[differ1 < 0] <- -differ1[differ1 < 0]
plot(differ1, main="Przekształcone straty S&P 500")
```

```{r echo=FALSE}
differ2<-diff(DJIA)[-1]
plot(differ2, main="Zróżnicowane ceny DJIA")
```

```{r echo=FALSE}
differ2 <- differ2[differ2 < 0]
plot(differ2, main="Straty DJIA")
```

```{r echo=FALSE}
differ2[differ2 < 0] <- -differ2[differ2 < 0]
plot(differ2, main="Przekształcone straty DJIA")
```

Mając już wyznaczone oba szeregi strat moge je ze sobą porównać:

```{r echo=FALSE}
par(mfrow = c(2, 1))
plot(differ1, main="Przekształcone straty S&P 500")
plot(differ2, main="Przekształcone straty DJIA")
```

Ponieważ same szeregi czasowe były podobne, to i również szeregi samych strat nie będą daleko od siebie odbiegały. Ważne jest jednak odnotowanie na tych wykresach, że oprócz różnej skali na osi Y, oś X również nie jest identyczna, co wynika z faktu że usuwając wszystkie dodatnie wartości ze zróżnicowanego szeregu zaburzyłem dzienną ciągłość danych, i sprawiłem że te szeregi nie mają równej długości. Najlepiej będzie to widać gdy przekształce je na zwykłe szeregi czasowe z numerycznymi indeksami:

```{r echo=FALSE}
par(mfrow = c(2, 1))
plot(as.ts(differ1), main="Przekształcone straty S&P 500", ylab="Wartość", xlab="Indeks")
plot(as.ts(differ2), main="Przekształcone straty DJIA", ylab="Wartość", xlab="Indeks")
```

Widać, że strat w przypadku S&P 500 jest więcej, natomiast ta różnica w ilości nie jest specjalnie duża, dlatego nadal mogę porównywać kształty tych wykresów. Tak samo jak dla oryginalnych szeregów czasowych są one bardzo podobne, czyli starty są ze sobą skorelowane.

Mając już wyznaczone szeregi strat wyświetlam ich podstawowe statystyki oraz wyznaczam ilość braków w danych, które interpretuje jako ilość danych usuniętych w poprzednich przeształcania oryginalnego zróżnicowania szeregu:

```{r echo=FALSE}
summary(differ1)
print_empty_line()
sapply(differ1, function(x) c(średnia = mean(x), odchylenie = sd(x), wariancja = var(x), moda =as.numeric(names(which.max(table(differ1)))), braki = (length(sp500_close)-length(differ1))))
```

```{r echo=FALSE}
summary(differ2)
print_empty_line()
sapply(differ2, function(x) c(średnia = mean(x), odchylenie = sd(x), wariancja = var(x), moda =as.numeric(names(which.max(table(differ2)))), braki = (length(DJIA)-length(differ2))))
```

Widać, że dla S&P 500 jest 15 więcej wartości niż dla DJIA. Kolejnym krokiem będzie sprawdzenie korelacji między tymi szeregami. Nie można jednak tego zrobić dla nierównych szeregów czasowych. Dlatego, by rozwiązać te nierówność między długościami w najbardziej "sprawiedliwy sposób", wyselekcjonuje z obu szeregów tylko te wartości, które mają swój odpowednik w drugim szeregu (inaczej mówiąc wezmę pod uwagę wartości tylko z dni, gdzie oba indeksy zanotowały stratę). Wpierw jednak muszę się upewnić, że nie zredukuje to zbytnio mojej ilości danych:

```{r echo=FALSE}
common_mask <- differ1 & differ2

aligned_loss1 <- differ1[common_mask]
aligned_loss2 <- differ2[common_mask]
print("Długość nowych szeregów strat:"); length(aligned_loss2)
print_empty_line()
print("Ilość straconych wartości dla dłuższego szeregu S&P 500:"); max(length(differ2)-length(aligned_loss2), length(differ1)-length(aligned_loss1))
```

Strata 67 wartości jest dosyć duża, ale nadal posiadam prawie 300 wartości do przeprowadzenia analizy, co nie wygląda niemożliwie do zrobienia. Sprawdzam więc korelacje między zmiennymi na podstawie współczynników:

```{r echo=FALSE}
print("Współczynnik Pearsona")
cor(aligned_loss1, aligned_loss2, method = "pearson")
print_empty_line()
print("Współczynnik Spearmana")
cor(aligned_loss1, aligned_loss2, method = "spearman")
print_empty_line()
print("Współczynnik Kendalla")
cor(aligned_loss1, aligned_loss2, method = "kendall")
```

Mimo tak podobych wykresów indeksów, jak i ich strat, każdy z współczynników wskazuje na nie aż tak znaczną korelacje między zmiennymi. Największa jest korelacja liniowa związana z współczynnikiem Pearsona, natomiast oczekiwałem tutaj wyniku znacznie bliżej jedynki. Tak samo dla zależności monotonicznej, czy według Spearmana czy Kendalla, wartość w moim odczuciu powinna wyjść większa. Sugeruje to, że albo ograniczenie się do samych strat, które jeszcze dodatkowo okroiłem, zbyt naruszyło zależności moich danych ze względu na zbyt małą próbke, albo że szeregi może i są podobne, ale wyizolowane straty i ich wartości nie są jednak tak samo zależne. 

## PKT 4

Kolejno dopasowuje do obu szeregów rozkład normalny i lognormalny i na podstawie wykresów diagnostycznych będe oceniał ich dopasowanie. 

Wpierw szereg strat indeksu S&P 500. Parametry rozkładu normalnego:

```{r echo=FALSE, warning=FALSE}
fitnorm1 <- fitdist(as.numeric(differ1), "norm")
fitnorm1$estimate
```

Wykresy diagnostyczne rozkładu normalnego:

```{r echo=FALSE}
plot(fitnorm1)
```

Sam histogram pokazuje, że dane nie przypominają rozkładu normalnego. Sam rozkład jest prawoskośny, z duża ilościa małych elementów koło 0 (czyli, że mam o wiele więcej drobnych strat niż załamań wartości). Pozostałe wykresy nie są również idealne, ale nie wyglądają tak źle, jak sugerowałby to histogram, co sugeruje, że rozkład może mieć jakiś związek z rozkładem normalnym (np. rozkład lognormalny).

Parametry rozkładu lognormalnego:

```{r echo=FALSE, warning=FALSE}
fitlnorm1 <- fitdist(as.numeric(differ1), "lnorm")
fitlnorm1$estimate
```

Wykresy diagnostyczne rozkładu lognormalnego:

```{r echo=FALSE}
plot(fitlnorm1)
```

Histogram, jak i funkcja gęstości o wiele bardziej sugerują, że nasze dane pochodzą z rozkładu lognormalnego. Dwa z trzech wykresów diagnostycznych znacznie poprawiły się względem rozkładu normalnego. Najbardziej jednak dziwi wykres kwantyl-kwantyl, który wygląda jakby był jakimś błędem. Nie wiem jednak z czego mogłoby to wynikać (chyba że ze zbyt małej liczby danych).

Następnie szereg strat indeksu DJIA. Parametry rozkładu normalnego:

```{r echo=FALSE, warning=FALSE}
fitnorm2 <- fitdist(as.numeric(differ2), "norm")
fitnorm2$estimate
```

Wykresy diagnostyczne rozkładu normalnego:

```{r echo=FALSE}
plot(fitnorm2)
```

Ponownie można wyciągnać te same wnioski, prawoskośny rozkład, histogram nie przypomina rozkładu normalnego, ale same wykresy diagnostyczne nie odstraszają swoim wyglądem. 

Parametry rozkładu lognormalnego:

```{r echo=FALSE, warning=FALSE}
fitlnorm2 <- fitdist(as.numeric(differ2), "lnorm")
fitlnorm2$estimate
```

Wykresy diagnostyczne rozkładu lognormalnego:

```{r echo=FALSE}
plot(fitlnorm2)
```

Ponownie wszystko poza wykresem kwantyl kwantyl sugeruje, że nasze dane pochodzą z rozkładu lognormalnego.

## PKT 5

Kolejnym zadaniem będzie dopasowywanie kolejnych typów kopuł do połączonych szeregów strat. Wpierw połączmy je w jeden obiekt:

```{r echo=FALSE}
aligned_loss1 <- coredata(aligned_loss1)
aligned_loss2 <- coredata(aligned_loss2)
u1 <- pobs(aligned_loss1)
u2 <- pobs(aligned_loss2)
data_matrix <- cbind(u1, u2)
head(data_matrix)
print_empty_line()
summary(data_matrix)
```

```{r include=FALSE}
gumbel_copula <- gumbelCopula(dim = 2)
frank_copula <- frankCopula(dim = 2)
clayton_copula <- claytonCopula(dim = 2)
normal_copula <- normalCopula(dim = 2)
t_copula <- tCopula(dim = 2, dispstr = "un")
```

A następnie po kolei dopasowujmy je do kopuły typu:

1. Gumbela:

```{r echo=FALSE}
fit_gumbel <- fitCopula(gumbel_copula, data_matrix)
summary(fit_gumbel)
```

2. Franka:

```{r echo=FALSE}
fit_frank <- fitCopula(frank_copula, data_matrix)
summary(fit_frank)
```

3. Claytona:

```{r echo=FALSE}
fit_clayton <- fitCopula(clayton_copula, data_matrix)
summary(fit_clayton)
```

4. Normalna:

```{r echo=FALSE}
fit_normal <- fitCopula(normal_copula, data_matrix)
summary(fit_normal)
```

5. T-studenta:

```{r echo=FALSE}
fit_t <- fitCopula(t_copula, data_matrix)
summary(fit_t)
```

Mając już wszystkie dopasowania sprawdzę, która dopasowana kopuła będzie najlepsza dla moich danych względem kolejnych kryteriów dopasowania modelu. Wpierw tworze osobne listy wartości dla danej kopuły dla każdego kryterium:

```{r echo=FALSE}
LL <- c(logLik(fit_gumbel),logLik(fit_frank),logLik(fit_clayton),logLik(fit_normal),logLik(fit_t))
names(LL) <- c("fit_gumbel", "fit_frank", "fit_clayton", "fit_normal", "fit_t")
print("Kryterium loglikelihood")
print(LL)
print_empty_line()
AIC <- c(AIC(fit_gumbel),AIC(fit_frank),AIC(fit_clayton),AIC(fit_normal),AIC(fit_t))
names(AIC) <- c("fit_gumbel", "fit_frank", "fit_clayton", "fit_normal", "fit_t")
print("Kryterium AIC")
print(AIC)
print_empty_line()
BIC <- c(BIC(fit_gumbel),BIC(fit_frank),BIC(fit_clayton),BIC(fit_normal),BIC(fit_t))
names(BIC) <- c("fit_gumbel", "fit_frank", "fit_clayton", "fit_normal", "fit_t")
print("Kryterium BIC")
print(BIC)
```

I końcowo wyznaczam najlepiej dopasowaną kopułę względem:

1. Kryterium loglikelihood(im większa wartość, tym lepiej dopasowany model):

```{r echo=FALSE}
names(LL)[which.max(LL)]; max(LL)
```

2. Kryterium AIC(im mniejsza wartość, tym lepiej dopasowany model):

```{r echo=FALSE}
names(AIC)[which.min(AIC)]; min(AIC)
```

3. Kryterium BIC(im mniejsza wartość, tym lepiej dopasowany model):

```{r echo=FALSE}
names(BIC)[which.min(BIC)]; min(BIC)
```

Jak widać wszystkie 3 kryteria wskazały, że najlepiej dopasowaną kopułą do moich danych jest kopuła Gumbella.

Ostatnią rzeczą będzie wykonanie testu Mardia dla moich danych, który zbada wielowymiarową normalność między moimi danymi:

```{r echo=FALSE}
result_mardia <- mardia(data_matrix)
print(result_mardia)
```

Z wykresu kwantyl-kwantyl mogę odczytać, że kwantyle moich danych pasują do teoretycznych kwantyli dwuwymiarowego rozkładu normalnego, z pewnymi niedopasowaniami na ogonach. Z wartości testu mogę odczytać duże p-value (tutaj napisanie jako probability) dla skośności moich danych, co daje przyjęcie hipotezy zerowej, że moje dane są symetryczne jak wielowymiarowy rozkład normalny. Natomiast p-value dla kurtozy jest już poniżej poziomu istotności 0.05, co powoduje odrzucenie hipotezy zerowej, że moje dane mają kutroze podobną do wielowymiarowego rozkład normalnego, zatem końcowo test sugeruje że mimo symetryczności mojego rozkładu nie jest on wielowymiarowym rozkładem normalnym. Ten wniosek zgadza się z cała poprzednią analizą moich zmiennych.

