---
title: "Analiza porównanawcza cen akcji NVIDIA i AMD - badanie parametrów"
author: "Mackiewicz-Kubiak Aleksander, Pągielska Marta"
date: "2025-06-12"
output:
  pdf_document: default
  html_document: default
---

## Pakiety

Do analizy użyto następujących pakietów:

```{r message=FALSE, warning=FALSE}
library(tidyr)
library(gamlss)
library(dplyr)
library(fitdistrplus)
library(usefun)
library(quantmod)
library(scales)
library(copula)
library(psych)
library(MVN)
library(readr)
library(xts)
options(scipen = 999) # wyłączenie notacji naukowej
set.seed(5463436)
```

## KROK 1

Do analizy wybrano ceny zamknięcia akcji NVIDIA i AMD z ostatnich 5 lat (pocżąwszy od 3 stycznia 2019 r. do 31 grudnia 2024 r. włącznie). Obie firmy działają w branży technologicznej i zajmują się produkcją procesorów i układów graficznych.

Ze względu na podobny profil działalności i działanie na tych samych rynkach, można przypuszczać, że ich ceny akcji mogą być ze sobą powiązane. Celem analizy jest sprawdzenie tej zależności.

Wczytanie danych z pliku CSV, konwersja formatu, utworzenie ramki danych oraz obiektu xts dla szeregów czasowych, podgląd danych:

```{r echo=FALSE}
data1 <- read.csv("nvd-amd.csv")
data1$Data <- as.Date(data1$Data, format = "%d.%m.%Y")

data1$NVDA <- as.numeric(gsub(",", ".", data1$NVDA))
data1$AMD <- as.numeric(gsub(",", ".", data1$AMD))

data1$NVDA <- as.numeric(gsub("[^0-9.-]", "", data1$NVDA))
data1$AMD <- as.numeric(gsub("[^0-9.-]", "", data1$AMD))

NVIDIA <- data.frame(Data = data1$Data, NVDA = data1$NVDA)
AMD <- data.frame(Data = data1$Data, AMD = data1$AMD)

NVIDIA <- xts(NVIDIA$NVDA, order.by = NVIDIA$Data)
colnames(NVIDIA) <- "NVIDIA.close"
AMD <- xts(AMD$AMD, order.by = AMD$Data)
colnames(AMD) <- "AMD.close"

head(AMD); head(NVIDIA)
```
Wizualizacja historycznych cen zamknięcia akcji dla NVIDIA oraz AMD:

```{r echo=FALSE}
par(mfrow = c(2, 1))
plot(NVIDIA, main="Ceny zamknięcia NVIDIA")
plot(AMD, main="Ceny zamknięcia AMD")
```

Ceny zamknięcia akcji NVIDIA wykazują wyraźny trend wzrostowy w analizowanym okresie. Szczególnie dynamiczny wzrost widoczny jest po roku 2022. Ceny akcji AMD mimo generalnej  tendecji wzrostowej, wykazują większą zmienność. Wzrost do 2022 roku był znaczący, ale od tego momentu ceny zaczęły spadać, a same wahania były większe niż w przypadku NVIDIA.

Transformacja danych poprzez obliczenie strat jako różnicy wartości zamknięcia między kolejnymi dniami oraz ich wizualizacja:

```{r echo=FALSE}
differ1<-diff(NVIDIA)[-1]
differ1 <- differ1[differ1 < 0]
differ1[differ1 < 0] <- -differ1[differ1 < 0]

differ2<-diff(AMD)[-1]
differ2 <- differ2[differ2 < 0]
differ2[differ2 < 0] <- -differ2[differ2 < 0]

par(mfrow = c(2, 1))
plot(as.ts(differ1), main="Przekształcone straty NVIDIA")
plot(as.ts(differ2), main="Przekształcone straty AMD")
```

Wykres przekształconych strat dla NVIDIA ukazuje stabilny okres do 2022 roku, po czym wzrost zmienności staje się bardziej zauważalny. Szczególnie intensywne wartości odchyleń widoczne są pod koniec 2024 roku. Wykres strat dla AMD jest bardziej dynamiczny niż dla NVIDIA, z wyraźnymi skokami w latach 2022-2024. Maksymalne straty osiągają wyższe wartości niż w przypadku NVIDIA.

## KROK 2

Problemem powyższych przekształceń jest fakt, że szeregi strat dla obu firm niekonieczne muszą być równej długości. Ponieważ naszym celem jest badanie zależności tych szeregów, należy zadbać by były one równej długości. W tym celu wyselekcjonujemy dane z dni, w których obie firmy zanotowały stratę, by te szeregi były jak najbardziej porównywalne, jednocześnie sprawdzając, czy nie usuniemy zbyt dużo danych.

```{r echo=FALSE}
common_mask <- differ1 & differ2

aligned_loss1 <- differ1[common_mask]
aligned_loss2 <- differ2[common_mask]
print("Długość nowych szeregów strat:"); length(aligned_loss2)
print_empty_line()
print("Ilość straconych wartości dla dłuższego szeregu"); max(length(differ2)-length(aligned_loss2), length(differ1)-length(aligned_loss1))
```

Nowe, równe szeregi mają wciąż dużą liczbe obserwacji, więc możemy przeprowadzać dla nich analizy.

Dopasowanie rozkładów:
Dopasowano rozkłady SEP4 (dla NVIDIA) oraz SEP3 (dla AMD). Wyniki wskazują na pewne problemy z dopasowaniem (np. ostrzeżenia o zbieżności). Mimo to, modele opisują asymetrię i różnorodność danych.

```{r include=FALSE}
fit1 <- fitDist(as.numeric(differ1),type="realline")
fit2 <- fitDist(as.numeric(differ2),type="realline")
```

Przetworzenie danych, by móc do nich dopasować kopuły:

```{r echo=FALSE}
aligned_loss1 <- coredata(aligned_loss1)
aligned_loss2 <- coredata(aligned_loss2)
u1 <- pobs(aligned_loss1)
u2 <- pobs(aligned_loss2)
data_matrix <- cbind(u1, u2)
head(data_matrix)
print_empty_line()
summary(data_matrix)
```

## KROK 3

Dopasowanie modeli kopuły:

```{r include=FALSE}
gumbel_copula <- gumbelCopula(dim = 2)
fit_gumbel <- fitCopula(gumbel_copula, data_matrix)
frank_copula <- frankCopula(dim = 2)
fit_frank <- fitCopula(frank_copula, data_matrix)
clayton_copula <- claytonCopula(dim = 2)
fit_clayton <- fitCopula(clayton_copula, data_matrix)
normal_copula <- normalCopula(dim = 2)
fit_normal <- fitCopula(normal_copula, data_matrix)
t_copula <- tCopula(dim = 2, dispstr = "un")
fit_t <- fitCopula(t_copula, data_matrix)
```

Kryteria dla dopasowanych kopuł:

```{r echo=FALSE}
LL <- c(logLik(fit_gumbel),logLik(fit_frank),logLik(fit_clayton),logLik(fit_normal),logLik(fit_t))
names(LL) <- c("fit_gumbel", "fit_frank", "fit_clayton", "fit_normal", "fit_t")
print("Kryterium loglikelihood")
print(LL)
print_empty_line()
AIC <- c(AIC(fit_gumbel),AIC(fit_frank),AIC(fit_clayton),AIC(fit_normal),AIC(fit_t))
names(AIC) <- c("fit_gumbel", "fit_frank", "fit_clayton", "fit_normal", "fit_t")
print("Kryterium AIC")
print(AIC)
print_empty_line()
BIC <- c(BIC(fit_gumbel),BIC(fit_frank),BIC(fit_clayton),BIC(fit_normal),BIC(fit_t))
names(BIC) <- c("fit_gumbel", "fit_frank", "fit_clayton", "fit_normal", "fit_t")
print("Kryterium BIC")
print(BIC)
```

Wyznaczenie najlepiej dopasowanej kopuły według różnych kryteriów:

```{r echo=FALSE}
names(LL)[which.max(LL)]; max(LL)
names(AIC)[which.min(AIC)]; min(AIC)
names(BIC)[which.min(BIC)]; min(BIC)
```

Kryteria AIC oraz BIC wybrały kopułę normalną jako najlepiej dopasowaną kopułę, podczas gdy loglikelihood wskazało kopułę t. Byłoby to problemem, jednak jak spojrzymy na wszystkie wartości kryterium loglielihood, to zauważymy, że różnica między kopułą t a normalną jest marginalna. Zatem można założyć, że kryteria wskazują kopułę normalną jako najlepiej dopasowaną kopułę. 

## PUNKT 4 i 5

W dalszej części analizy przyjmujemy inne podejście niż dotychczas, ponnieważ zamiast skupiać się na pojedynczych wynikach lub konkretnej konfiguracji portfela, konstruujemy jedną zbiorczą ramkę danych results_df, która zawiera rezultaty uzyskane dla wielu portfeli i różnych parametrów. Każdy wiersz tej ramki odpowiada jednej kombinacji parametrów (np. typu portfela, sposobu generowania danych, metody optymalizacji), a kolumny zawierają najważniejsze statystyki opisujące wynik, w tym współczynnik Beta, wartość Alpha oraz typ danych wejściowych. Dzięki temu możemy podejść do analizy bardziej przekrojowo: badać rozkład współczynnika Beta wśród różnych podejść, identyfikować nietypowe przypadki (np. gdy Beta osiąga wartości graniczne), czy też porównywać skuteczność różnych metod modelowania ryzyka. Takie ujęcie pozwala nie tylko lepiej zrozumieć właściwości poszczególnych rozwiązań, ale także wychwycić ogólne trendy, nieoczywiste zależności oraz potencjalne błędy lub niespójności w analizie. W kolejnych krokach skoncentrujemy się na eksploracji tej ramki danych — rozpoczynając od oceny rozkładu współczynnika Beta, a następnie filtrując i porównując przypadki najbardziej charakterystyczne lub skrajne.

```{r include=FALSE}
compute_var <- function(beta, x, alpha = 0.05) {
  portfolio <- beta * x[,1] + (1 - beta) * x[,2]
  return(quantile(portfolio, probs = alpha))
}
```

Nazwa ramki danych - results_df
Parametry:
Data-dane użyte do obliczeń
Type-rodzaj funkcji użyty do obliczeń
Betaseq-długość przedziałów wag między [0,1], dla których obliczamy najmniejszy VaR
Alpha-poziom istotności wyliczanego VaRu
N-liczba losowanych obserwacji w kopule/rozkładach
VaR-wyliczone wartości Value at Risk
Beta - optymalna waga, dla której wartość VaR była możliwie najmniejsza

Pętla generuje N-danych z dopasowanych rozkładów brzegowych (opdasowanych za pomocą fitDist) oraz z najlepiej dopasowanej kopuły (wybranej przy użyciu kryterium porównawczego AIC). Dla obu tych zbiorów danych używamy dwóch funkcji, wbudowanej w R funkcji optimize, i ręcznie napisanej compute_var, by dla konkretnego poziomu istotności ($\alpha$) obliczyć dla jakiej wagi Beta z przedziału [0,1] wartość VaR jest najmniejsza. Funkcja optimize automatycznie ustala przedziały wartości Beta, a dla compute_var trzeba to ustalić ręcznie, za pomocą parametru Betaseq. Dla każdej wagi beta obliczamy VaR takiego portfela, a następnie wybieramy tylką tą wartość beta, dla której wartość VaR jest najmniejsza. Końcowo, tak obliczone wartości dodajemy do tabelki results_df, razem ze wszystkimi parametrami użytymi do ich wyliczenia. Dodawanie odbywa się czterokrotnie, gdyż korzystamy z dwóch rodzajóW danych i dwóch rodzajów funkcji, co daje cztery osobne wyniki w jednej iteracji pętli, każdy z różnych parametrami typu characteristic. Jako wisienka na torcie upewniamy się, że numeracja rzędów jest bazowa, dla estetyki i praktyczności tej tabeli. Łącznie pętla przechodzi 4x3x7=84 iteracje, co daje 336 operacji dodawania do tabeli, i sprawia że nasza tabela będzie miała wymiary 336x7. 

```{r}
values_N <- c(50, 100, 500, 1000, 5000, 10000, 50000)
values_Alpha <- c(0.9, 0.95, 0.99) 

lista1 <- c(seq(0, 1, by = 0.1))
lista2 <- c(seq(0, 1, by = 0.01))
lista3 <- c(seq(0, 1, by = 0.001))
lista4 <- c(seq(0, 1, by = 0.0001))
betaslist <- list(lista1, lista2, lista3, lista4)

results_df <- data.frame(Data = character(), Type=character(), Betaseq = numeric(), 
                         Alpha = numeric(), N = numeric(), VaR = numeric(), Beta=numeric(), Difference = numeric())
```

```{r echo=FALSE}
for(betas in betaslist){
  for (alp in values_Alpha){
    for (N in values_N){
  
      simulated_values1 <- do.call(paste0("r", fit1$family[1]), c(list(N), fit1[fit1$parameters]))
      simulated_values2 <- do.call(paste0("r", fit2$family[1]), c(list(N), fit2[fit2$parameters]))
      simulated_values_dist <- matrix(c(simulated_values1, simulated_values2), nrow = N, ncol = 2)
      
      simulated_values_copula <- rCopula(N, get(names(AIC)[which.min(AIC)])@copula)
      
      simulated_values1 <- do.call(paste0("q", fit1$family[1]), c(list(simulated_values_copula[, 1]), fit1[fit1$parameters]))
      simulated_values2 <- do.call(paste0("q", fit2$family[1]), c(list(simulated_values_copula[, 2]), fit2[fit2$parameters]))
      simulated_values_copula_changed <- data.frame(x1 = simulated_values1, x2 = simulated_values2)
  
      result_dist <- optimize(compute_var, interval = c(0, 1), x = simulated_values_dist, alpha = alp)
      var_results1 <- data.frame(Beta = numeric(length(betas)), VaR = numeric(length(betas)))
  
      for (i in seq_along(betas)) {
        var_results1[i, ] <- c(betas[i], compute_var(betas[i], x = simulated_values_dist, alp))
      }
      sorted_var_results1 <- var_results1[order(var_results1$VaR), ]
      
      result_copula <- optimize(compute_var, interval = c(0, 1), x = simulated_values_copula_changed, alpha = alp)
      var_results2 <- data.frame(Beta = numeric(length(betas)) ,VaR = numeric(length(betas)))
  
      for (i in seq_along(betas)) {
        var_results2[i, ] <- c(betas[i], compute_var(betas[i], x = simulated_values_copula_changed, alp))
      }
      
      sorted_var_results2 <- var_results2[order(var_results2$VaR), ]
      
      results_df <- rbind(results_df, data.frame(Data="Empiric", Type="Optimize", Betaseq=betas[2]-betas[1], Alpha=alp, N=N, VaR=result_dist$objective, 
                                                 Beta=result_dist$minimum))
      results_df <- rbind(results_df, data.frame(Data="Empiric", Type="Own method", Betaseq=betas[2]-betas[1], Alpha=alp, N=N, 
                                                 VaR=sorted_var_results1$VaR[1], Beta=sorted_var_results1$Beta[1]))
      results_df <- rbind(results_df, data.frame(Data="Copula", Type="Optimize", Betaseq=betas[2]-betas[1], Alpha=alp, N=N, VaR=result_copula$objective, 
                                                 Beta=result_copula$minimum))
      results_df <- rbind(results_df, data.frame(Data="Copula", Type="Own method", Betaseq=betas[2]-betas[1], Alpha=alp, N=N, 
                                                 VaR=sorted_var_results2$VaR[1], Beta=sorted_var_results2$Beta[1]))
    }
  }
}
rownames(results_df) <- NULL
```

W wyniku działania powyższej pętli tworzymy tabelkę danych results_df mającą 7 kolumn, które możemy "podzielić" na 3 typy. Pierwsze dwie kolumny (Data, Type) to kolumny binarne, zawierające informacje o danych i metodzie użytej do liczenia Varu. W moim przypadku mamy podział na dane z dopasowanych rozkładów empirycznych (Data=Empiric) i na dane z dopasowanej kopuły (Data=Copula), oraz na metoda liczenia za pomocą gotowej funkcji w R (Type=Optimize) i recznie napisanej analogicznej funkcji (Type=Own method). Kolejnym typem danych są parametry: N mówiący o liczbie próbek losowanych z dopasowanych rozkładów i kopuł, oraz Betaseq mówiący o długości przedziału wartości Beta dla których badaliśmy najmniejszy możliwy Var. Trzecim typem danych są dane które docelowo mieliśmy policzyć, Beta mówiący o wartości parametru dla którego minimalizujemy VaR dla naszego portfela akcji Nvidia i AMD, i VaR podający wartość liczbową najmniejszego Varu. Do tego można dopisać wartości Alpha, które odpowiadają różnym poziomom VaRu (gdyż VaR możemy liczyć dla dowolnej Alphy z przedziału 0-1, którą interpretuje się jako prawdopobieństwo. Wartości Alpha, Beta i VaR są zatem bezpośrednio ze sobą powiązane, 

```{r echo=FALSE}
print("Korelacja Beta-VaR")
cat("Pearsona:", cor(results_df$VaR, results_df$Beta, method = "pearson"))
print_empty_line()
cat("Spearmana:", cor(results_df$VaR, results_df$Beta, method = "spearman"))
print_empty_line()
print("Korelacja Beta-Alpha")
cat("Pearsona:", cor(results_df$Alpha, results_df$Beta, method = "pearson"))
print_empty_line()
cat("Spearmana:", cor(results_df$Alpha, results_df$Beta, method = "spearman"))
print_empty_line()
print("Korelacja Alpha-VaR")
cat("Pearsona:", cor(results_df$Alpha, results_df$VaR, method = "pearson"))
print_empty_line()
cat("Spearmana:", cor(results_df$Alpha, results_df$VaR, method = "spearman"))
```

i ponieważ one były celem w oryginalnej analizie, to nim przyjrzę się najbardziej. Na pierwszy ogień bierzemy wartości VaRu. Wpierw histogram zebranych wartości: 

```{r echo=FALSE}
hist(results_df$VaR)
```

Rozkład ma wartości w przedziale 0 do 7. Jest on bardzo nieregularnie rozłożony, co może być kwestią małej liczby danych, lub dowodem na to, że te metody obliczeniowe nie były idealne. By to zweryfikować, sprawdzimy pewną zależność, a mianowicie czy dla różne poziomy Alpha wartości VaRu na siebie nie nachodzą.

```{r echo=FALSE}
p1 <- hist(results_df$VaR[results_df$Alpha==0.95])
p2 <- hist(results_df$VaR[results_df$Alpha==0.90])                
plot( p1, col=rgb(0,0,1,1/4)) 
plot( p2, col=rgb(1,0,0,1/4), add=T) 
```

```{r echo=FALSE}
p1 <- hist(results_df$VaR[results_df$Alpha==0.99])
p2 <- hist(results_df$VaR[results_df$Alpha==0.95])                
plot( p1, col=rgb(0,0,1,1/4)) 
plot( p2, col=rgb(1,0,0,1/4), xlim=c(0,10), add=T) 
```

Wykresy na pierwszy rzut oka wyglądają jakby faktycznie dla wyższych poziomów Alpha Wartości VaRu były większe, wręcz przesunięte na wykresie o stałą wartość. Widać jednak, że są miejsca na wykresie na których wartości na siebie nachodzą, i to nawet niekoniecznie pomiędzy dominantami w tych wykresach. Sugeruje to, że nie wszystkie metody czy parametry zastosowane do obliczania VaRu były odpowiednie. Zobaczmy średnią wartość VaRu dla konkretnych użytych danych i funkcji.

```{r echo=FALSE}
mean(results_df$VaR[results_df$Type=="Own method" & results_df$Data=="Copula"])
mean(results_df$VaR[results_df$Type=="Optimize" & results_df$Data=="Empiric"])
mean(results_df$VaR[results_df$Type=="Own method" & results_df$Data=="Empiric"])
mean(results_df$VaR[results_df$Type=="Optimize" & results_df$Data=="Copula"])
```

Wartości średnich są mocno zbliżone, co sugeruje, że wybór metody i danych użytych do obliczeń był poprawny. Zobaczmy, jak to wygląda dla wartości Beta.

Ponownie, wpierw zacznę od histogramu, pamiętając, że Beta może przyjmować wartości z przedziału [0,1]:

```{r echo=FALSE}
hist(results_df$Beta)
```

Rozkład jest mocno lewoskośny, z największym skupiskiem wartości przy możliwie maksymalnej wartości Beta równej 1. Co więcej wszystkie wartości Beta wydają się być powyżej wartości graniczej 0.5. Upewniamy się, że tak jest: 

```{r echo=FALSE}
min(results_df$Beta); length(results_df[results_df$Beta <= 0.5])
```

Najmniejsza wartość Beta jest powyżej 0.5. Oznacza to, że w każdym wyniku jeden portfel, konkretniej AMD, jest bardziej stabliny przez co ma mniejszą wartość VaR. Zatem nawet jeżeli część wynikóW jest nieprecyzyjna, co można również wnioskować ze skośności rozkładu wartości Beta, to nie jest na tyle nieprecyzyjna by zaburzać informacje który portfel jest tym mniej ryzykownym, co waliduje wcześniejszą analize. 

Przyjrzymy się teraz przypadkom, w którym Beta wyniosła jeden, gdyż w przypadku akcji Nvidii i AMD nie oczekiwalibyśmy tak mocnej polaryzacji w kierunku jednego portfela, stąd te wartości najszybciej rzucąją się w oczy jako to potencjalnie błędne.

```{r echo=FALSE}
filter(results_df, results_df$Beta == 1)
```

Można zaobserwować, że niektóre wartości w tej tabeli pojawiąją się zdecydowanie częściej, a z kolei inne nie pojawiają sie ani razu. By to dokładniej zobaczyć, zrobimy osobne podziały dla każdej kolumny.

```{r echo=FALSE}
results_df %>%
  filter(Beta == 1) %>%
  count(Type) 

results_df %>%
  filter(Beta == 1) %>%
  count(Data) 

results_df %>%
  filter(Beta == 1) %>%
  count(Betaseq) 

results_df %>%
  filter(Beta == 1) %>%
  count(N) 

results_df %>%
  filter(Beta == 1) %>%
  count(Alpha) 
```

Z powyższych list można zauważyc, że wartości Beta=1 zachodzą niezależnie od danych, których użyliśmy do liczenia VaRu, ale zachodzą tylko dla ręcznej metody liczenia. Co więcej, zachodzą one najczęściej dla najmniejszych przyjętych wartości Alpha i dla największej długości przedziału Betaseq, czyli bazując na wiedzy empirycznej, dla możliwie najmniej rzetelnych parametrów. Jest zatem podstawa, by sądzić że tylko dla odpowiednio dobranych Type, Alpha i Betaseq wartości są wiarygodne.

Możemy jeszcze przyjrzeć się różnicom wartości VaR dla różnych metod i parametrów. Wpierw porównamy reszty dla dwóch różnych funkcji, których użyliśmy: 

```{r echo=FALSE}
diff1<-results_df$VaR[results_df$Type=="Optimize"]-results_df$VaR[results_df$Type=="Own method"]
summary(diff1)
sd(diff1); var(diff1); as.numeric(names(which.max(table(diff1))))
hist(diff1)
```

Wartości reszt wachają się między -0.3 a 0.9. Średnie wartości VaRu, które wcześniej sprawdzaliśmy, oscylowały w okolicach 3.5, zatem te reszty nie wydają się być duże. Dodatkowo, na histogramie widać bardzo dużą ilość reszt w okolicach wartości 0, co sugeruje, że obie funkcje dawały bardzo porównywalne wyniki. 

Teraz porównany reszty wartości VaR dla dwóch różnych rodzajów danych, które stosowaliśmy do obliczeń, dane empiryczne i dane kopułowe:

```{r echo=FALSE}
diff2<-results_df$VaR[results_df$Data=="Empiric"]-results_df$VaR[results_df$Data=="Copula"]
summary(diff2)
sd(diff2); var(diff2); as.numeric(names(which.max(table(diff2))))
hist(diff2)
```

Różnice w tym przypadku są zdecydowanie większe, niż dla różnych metod. O ile ponownie większość reszt ma wartości bliskie zeru, tak istnieją wartości odstające przekraczające średnie wartości VaRu, co sugeruje, że różne dane w niektórych przypadkach dawały skrajnie różne wyniki. 

Możemy zastosować podobną analize dla różnich w wartościach Beta, mając na uwadze jej ograniczony przedział możliwych wartości [0,1].

```{r echo=FALSE}
diffb1<-results_df$Beta[results_df$Type=="Optimize"]-results_df$Beta[results_df$Type=="Own method"]
summary(diffb1)
sd(diffb1); var(diffb1); as.numeric(names(which.max(table(diffb1))))
hist(diffb1)
```

Rozkład jest lewoskośny, z dominującą liczbą wartości bliskich zero. 

```{r echo=FALSE}
diffb2<-results_df$Beta[results_df$Data=="Empiric"]-results_df$Beta[results_df$Data=="Copula"]
summary(diffb2)
sd(diffb2); var(diffb2); as.numeric(names(which.max(table(diffb2))))
hist(diffb2)
```

Rozkład wygląda na symetryczny, łudząco przypominającyw rozkład normalny.

Rozkład reszt wartości Beta dla dwóch różnych funkcji jest "ładniejszym" rozkładem, ale z większym przedziałem możliwych wartości, w porównaniu do tego badającego reszty dla różnych danych. Pokrywa się to z poprzednimi obserwacjami, gdzie tylko dla ręcznie napisanej funkcji pojawiały się wartości Beta=1, które mogły zaburzyć rozkład reszt i dodać mu lewoskośności. 

Zatem końcowo można wysnuć wnioski, że:
-wybór danych pomiędzy tymi z kopuły a tymi z rozkładów wpływał mocno na wartości VaR 
-wybór funkcji do obliczeń wpływał mocno na wartości Beta 
-dobór odpowiednich parametrów N, Betaseq i Alpha miał znaczenie: im większa Alpha i N tym bardziej wiarygodne wyniki, im mniejszy przedział Betaseq tym wiarygodniejsze wyniki

