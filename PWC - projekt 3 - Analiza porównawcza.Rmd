---
title: "Analiza porównanawcza cen akcji NVIDIA i AMD"
author: "Mackiewicz-Kubiak Aleksander, Pągielska Marta"
date: "2024-01-10"
output:
  pdf_document: default
  html_document: default
---

## Pakiety

Do analizy użyto następujących pakietów:

```{r message=FALSE, warning=FALSE}
library(tidyr)
library(gamlss)
library(dplyr)
library(fitdistrplus)
library(usefun)
library(quantmod)
library(scales)
library(copula)
library(psych)
library(MVN)
library(readr)
library(xts)
options(scipen = 999) # wyłączenie notacji naukowej
```

## KROK 1

Do analizy wybrano ceny zamknięcia akcji NVIDIA i AMD z ostatnich 5 lat (pocżąwszy od 3 stycznia 2019 r. do 31 grudnia 2024 r. włącznie). Obie firmy działają w branży technologicznej i zajmują się produkcją procesorów i układów graficznych.

Ze względu na podobny profil działalności i działanie na tych samych rynkach, można przypuszczać, że ich ceny akcji mogą być ze sobą powiązane. Celem analizy jest sprawdzenie tej zależności.

Wczytanie danych z pliku CSV, konwersja formatu, utworzenie ramki danych oraz obiektu xts dla szeregów czasowych, podgląd danych:

```{r echo=FALSE}
data <- read.csv("nvd-amd.csv")
data$Data <- as.Date(data$Data, format = "%d.%m.%Y")

data$NVDA <- as.numeric(gsub(",", ".", data$NVDA))
data$AMD <- as.numeric(gsub(",", ".", data$AMD))

data$NVDA <- as.numeric(gsub("[^0-9.-]", "", data$NVDA))
data$AMD <- as.numeric(gsub("[^0-9.-]", "", data$AMD))

NVIDIA <- data.frame(Data = data$Data, NVDA = data$NVDA)
AMD <- data.frame(Data = data$Data, AMD = data$AMD)

NVIDIA <- xts(NVIDIA$NVDA, order.by = NVIDIA$Data)
colnames(NVIDIA) <- "NVIDIA.close"
AMD <- xts(AMD$AMD, order.by = AMD$Data)
colnames(AMD) <- "AMD.close"

head(AMD); head(NVIDIA)
```

Wizualizacja historycznych cen zamknięcia akcji dla NVIDIA oraz AMD:

```{r echo=FALSE}
par(mfrow = c(2, 1))
plot(NVIDIA, main="Ceny zamknięcia NVIDIA")
plot(AMD, main="Ceny zamknięcia AMD")
```

Ceny zamknięcia akcji NVIDIA wykazują wyraźny trend wzrostowy w analizowanym okresie. Szczególnie dynamiczny wzrost widoczny jest po roku 2022. Ceny akcji AMD mimo generalnej  tendecji wzrostowej, wykazują większą zmienność. Wzrost do 2022 roku był znaczący, ale od tego momentu ceny zaczęły spadać, a same wahania były większe niż w przypadku NVIDIA.

Transformacja danych poprzez obliczenie strat jako różnicy wartości zamknięcia między kolejnymi dniami oraz ich wizualizacja:

```{r echo=FALSE}
differ1<-diff(NVIDIA)[-1]
differ1 <- differ1[differ1 < 0]
differ1[differ1 < 0] <- -differ1[differ1 < 0]

differ2<-diff(AMD)[-1]
differ2 <- differ2[differ2 < 0]
differ2[differ2 < 0] <- -differ2[differ2 < 0]

par(mfrow = c(2, 1))
plot(differ1, main="Przekształcone straty NVIDIA")
plot(differ2, main="Przekształcone straty AMD")
```

Wykres przekształconych strat dla NVIDIA ukazuje stabilny okres do 2022 roku, po czym wzrost zmienności staje się bardziej zauważalny. Szczególnie intensywne wartości odchyleń widoczne są pod koniec 2024 roku. Wykres strat dla AMD jest bardziej dynamiczny niż dla NVIDIA, z wyraźnymi skokami w latach 2022-2024. Maksymalne straty osiągają wyższe wartości niż w przypadku NVIDIA.

## KROK 2

```{r echo=FALSE}
par(mfrow = c(2, 1))
plot(as.ts(differ1), main="Przekształcone straty NVIDIA", ylab="Wartość", xlab="Indeks")
plot(as.ts(differ2), main="Przekształcone straty AMD", ylab="Wartość", xlab="Indeks")
```

Obliczenie podstawowych statystyk:

```{r echo=FALSE}
summary(differ1)
print_empty_line()
sapply(differ1, function(x) c(średnia = mean(x), odchylenie = sd(x), wariancja = var(x), moda =as.numeric(names(which.max(table(differ1)))), braki = (length(NVIDIA)-length(differ1))))

summary(differ2)
print_empty_line()
sapply(differ2, function(x) c(średnia = mean(x), odchylenie = sd(x), wariancja = var(x), moda =as.numeric(names(which.max(table(differ2)))), braki = (length(AMD)-length(differ2))))
```

AMD cechuje się większym odchyleniem standardowym i większą wariancją niż NVIDIA. Zatem straty AMD są średnio większe i bardziej zmienne, co wskazuje na większe ryzyko związane z akcjami AMD.

Problemem powyższych przekształceń jest fakt, że szeregi strat dla obu firm niekonieczne muszą być równej długości. Ponieważ naszym celem jest badanie zależności tych szeregów, należy zadbać by były one równej długości. W tym celu wyselekcjonujemy dane z dni, w których obie firmy zanotowały stratę, by te szeregi były jak najbardziej porównywalne, jednocześnie sprawdzając, czy nie usuniemy zbyt dużo danych.

```{r echo=FALSE}
common_mask <- differ1 & differ2

aligned_loss1 <- differ1[common_mask]
aligned_loss2 <- differ2[common_mask]
print("Długość nowych szeregów strat:"); length(aligned_loss2)
print_empty_line()
print("Ilość straconych wartości dla dłuższego szeregu"); max(length(differ2)-length(aligned_loss2), length(differ1)-length(aligned_loss1))
```

Nowe, równe szeregi mają wciąż dużą liczbe obserwacji, więc możemy przeprowadzać dla nich analizy.

Dopasowanie rozkładów:
Dopasowano rozkłady SEP4 (dla NVIDIA) oraz SEP3 (dla AMD). Wyniki wskazują na pewne problemy z dopasowaniem (np. ostrzeżenia o zbieżności). Mimo to, modele opisują asymetrię i różnorodność danych.

```{r include=FALSE}
fit1 <- fitDist(as.numeric(differ1),type="realline")
```

```{r echo=FALSE}
fit1
plot(fit1)
```

Rozrzut reszt wskazuje na pewne odchylenia, ale większość punktów skupia się blisko wartości dopasowanych, co sugeruje całkiem niezłe dopasowanie modelu. Pewne niespójności modelu może sugerować nieregularne rozłożenie reszt w czasie. Odchylenia reszt od linii prostej na wykresie Q-Q sugerują nienormalność rozkładu, szczególnie w ogonach.

```{r include=FALSE}
fit2 <- fitDist(as.numeric(differ2),type="realline")
```

```{r echo=FALSE}
fit2
plot(fit2)
```

Reszty są skupione wokół bardzo wąskiego zakresu wartości dopasowanych (około 0.01), co wskazuje na dużą zgodność między modelem a danymi w tym zakresie. Rozkład jest dość równomierny. Wygląda na to, że reszty są losowe w czasie. W skrajnych wartość następuje odchylenie od rozkładu normalnego.

Obliczenie korelacji dla przekształconych strat i pełnych danych cenowych:

```{r echo=FALSE}
print("Współczynnik Pearsona")
cor(aligned_loss1, aligned_loss2, method = "pearson")
print_empty_line()
print("Współczynnik Spearmana")
cor(aligned_loss1, aligned_loss2, method = "spearman")
print_empty_line()
print("Współczynnik Kendalla")
cor(aligned_loss1, aligned_loss2, method = "kendall")
```

Korelacja dla przekształconych strat jest niska, co sugeruje, że spadki cen nie mają ze sobą znaczącej zależności liniowej.

```{r echo=FALSE}
print("Współczynnik Pearsona")
cor(NVIDIA, AMD, method = "pearson")
print_empty_line()
print("Współczynnik Spearmana")
cor(NVIDIA, AMD, method = "spearman")
print_empty_line()
print("Współczynnik Kendalla")
cor(NVIDIA, AMD, method = "kendall")
```

Mimo słabej korelacji między stratami, korelacja dla cen zamknięcia jest bardzo wysoka, co wskazuje na dodatnią zależność liniową trendów obu firm.

## KROK 3

Dopasowanie modeli kopuły:

```{r echo=FALSE}
aligned_loss1 <- coredata(aligned_loss1)
aligned_loss2 <- coredata(aligned_loss2)
u1 <- pobs(aligned_loss1)
u2 <- pobs(aligned_loss2)
data_matrix <- cbind(u1, u2)
head(data_matrix)
print_empty_line()
summary(data_matrix)
```

1. Gumbela:

```{r include=FALSE}
gumbel_copula <- gumbelCopula(dim = 2)
frank_copula <- frankCopula(dim = 2)
clayton_copula <- claytonCopula(dim = 2)
normal_copula <- normalCopula(dim = 2)
t_copula <- tCopula(dim = 2, dispstr = "un")
```

```{r echo=FALSE}
fit_gumbel <- fitCopula(gumbel_copula, data_matrix)
summary(fit_gumbel)
```

2. Franka:

```{r echo=FALSE}
fit_frank <- fitCopula(frank_copula, data_matrix)
summary(fit_frank)
```

3. Claytona:

```{r echo=FALSE}
fit_clayton <- fitCopula(clayton_copula, data_matrix)
summary(fit_clayton)
```

4. Normalna:

```{r echo=FALSE}
fit_normal <- fitCopula(normal_copula, data_matrix)
summary(fit_normal)
```

5. T-studenta:

```{r echo=FALSE}
fit_t <- fitCopula(t_copula, data_matrix)
summary(fit_t)
```

```{r echo=FALSE}
LL <- c(logLik(fit_gumbel),logLik(fit_frank),logLik(fit_clayton),logLik(fit_normal),logLik(fit_t))
names(LL) <- c("fit_gumbel", "fit_frank", "fit_clayton", "fit_normal", "fit_t")
print("Kryterium loglikelihood")
print(LL)
print_empty_line()
AIC <- c(AIC(fit_gumbel),AIC(fit_frank),AIC(fit_clayton),AIC(fit_normal),AIC(fit_t))
names(AIC) <- c("fit_gumbel", "fit_frank", "fit_clayton", "fit_normal", "fit_t")
print("Kryterium AIC")
print(AIC)
print_empty_line()
BIC <- c(BIC(fit_gumbel),BIC(fit_frank),BIC(fit_clayton),BIC(fit_normal),BIC(fit_t))
names(BIC) <- c("fit_gumbel", "fit_frank", "fit_clayton", "fit_normal", "fit_t")
print("Kryterium BIC")
print(BIC)
```

Wyznaczenie najlepiej dopasowanej kopuły względem:

1. Kryterium loglikelihood(im większa wartość, tym lepiej dopasowany model):

```{r echo=FALSE}
names(LL)[which.max(LL)]; max(LL)
```

2. Kryterium AIC(im mniejsza wartość, tym lepiej dopasowany model):

```{r echo=FALSE}
names(AIC)[which.min(AIC)]; min(AIC)
```

3. Kryterium BIC(im mniejsza wartość, tym lepiej dopasowany model):

```{r echo=FALSE}
names(BIC)[which.min(BIC)]; min(BIC)
```

Kryteria AIC oraz BIC wybrały kopułę normalną jako najlepiej dopasowaną kopułę, podczas gdy loglikelihood wskazało kopułę t. Byłoby to problemem, jednak jak spojrzymy na wszystkie wartości kryterium loglielihood, to zauważymy, że różnica między kopułą t a normalną jest marginalna. Zatem można założyć, że kryteria wskazują kopułę normalną jako najlepiej dopasowaną kopułę. 

Wykonanie testu Mardia:

```{r echo=FALSE}
mvn(data = as.matrix(data.frame(aligned_loss1, aligned_loss2)), mvnTest = "mardia")
```

Jak widać test Mardia wskazuje, że nasze dane nie są z dwuwymiarowego rozkładu normalnego.

Wizualizacja najlepiej dopasowanej kopuły:

```{r echo=FALSE}
par(mfrow = c(1, 2))
simulated_copula <- rCopula(nrow(data_matrix), fit_normal@copula)

u1 <- seq(0, 1, length.out = 100)
u2 <- seq(0, 1, length.out = 100)
grid <- expand.grid(u1, u2)

contour(u1, u2, matrix(dCopula(as.matrix(grid), fit_normal@copula), nrow = 100), 
        main = "Wykres konturowy kopuły", 
        xlab = "U1", ylab = "U2", col = "blue")

persp(fit_gumbel@copula, dCopula, main = "Dopasowana kopuła Normalna")
```

## PUNKT 4 i 5

Obliczono wartość zagrożoną (VaR) portfela składającego się z dwóch składników, korzystając zarówno z danych rzeczywistych, jak i danych wygenerowanych na podstawie najlepiej dopasowanej kopuły. Najpierw generowane są dane z dopasowanych rozkładów brzegowych, a później dane z kopuły. Następnie dla każdej próbki i poziomu istotności ($\alpha$) obliczane są optymalne wagi ($\beta$), które minimalizują ryzyko portfela. Obliczenia są wykonywane na dwa sposoby: za pomocą funkcji optymalizującej (optimize) oraz iteracyjnie, aby upewnić się, że wyniki są spójne i aby móc porównać ich dokładność w różnych sytuacjach. 

```{r include=FALSE}
compute_var <- function(beta, x, alpha = 0.05) {
  portfolio <- beta * x[,1] + (1 - beta) * x[,2]
  return(quantile(portfolio, probs = alpha))
}
```

```{r echo=FALSE}
values_N <- c(500, 1000)
values_Alpha <- c(0.95, 0.99)  
betas <- seq(0, 1, by = 0.0001)
set.seed(5463436)

for (N in values_N) {
  for (alp in values_Alpha) {

    simulated_values1 <- do.call(paste0("r", fit1$family[1]), c(list(N), fit1[fit1$parameters]))
    simulated_values2 <- do.call(paste0("r", fit2$family[1]), c(list(N), fit2[fit2$parameters]))
    simulated_values_dist <- matrix(c(simulated_values1, simulated_values2), nrow = N, ncol = 2)
    
    simulated_values_copula <- rCopula(N, get(names(AIC)[which.min(AIC)])@copula)
    
    simulated_values1 <- do.call(paste0("q", fit1$family[1]), c(list(simulated_values_copula[, 1]), fit1[fit1$parameters]))
    simulated_values2 <- do.call(paste0("q", fit2$family[1]), c(list(simulated_values_copula[, 2]), fit2[fit2$parameters]))
    simulated_values_copula_changed <- data.frame(x1 = simulated_values1, x2 = simulated_values2)

    result_dist <- optimize(compute_var, interval = c(0, 1), x = simulated_values_dist, alpha = alp)
    cat("Wartości Beta (z funkcji optimize) z danych empirycznych dla N =", N, "i Alpha =", alp, 
        "wynosi", round(result_dist$minimum,6))
    print_empty_line()
    
    var_results1 <- data.frame(Beta = numeric(length(betas)), VaR = numeric(length(betas)))

    for (i in seq_along(betas)) {
      var_results1[i, ] <- c(betas[i], compute_var(betas[i], x = simulated_values_dist, alp))
    }
    sorted_var_results1 <- var_results1[order(var_results1$VaR), ]
    
    cat("Wartości Beta (liczona ręcznie) z danych empirycznych dla N =", N, "i Alpha =", alp, "wynosi", round(sorted_var_results1$Beta[1],6), "\n")
    
    result_copula <- optimize(compute_var, interval = c(0, 1), x = simulated_values_copula_changed, alpha = alp)
    cat("Wartości Beta (z funkcji optimize) z danych z kopuły dla N =", N, "i Alpha =", alp, 
        "wynosi", round(result_copula$minimum,6))
    print_empty_line()

    var_results2 <- data.frame(Beta = numeric(length(betas)) ,VaR = numeric(length(betas)))

    for (i in seq_along(betas)) {
      var_results2[i, ] <- c(betas[i], compute_var(betas[i], x = simulated_values_copula_changed, alp))
    }
    
    sorted_var_results2 <- var_results2[order(var_results2$VaR), ]

    cat("Wartości Beta (liczona ręcznie) z danych z kopuły dla N =", N, "i Alpha =", alp, "wynosi", round(sorted_var_results2$Beta[1],6), "\n")
    print_empty_line()
    
  }
}
```

```{r echo=FALSE}
values_N <- c(500, 1000)
values_Alpha <- c(0.95, 0.99)  
betas <- seq(0, 1, by = 0.0001)
set.seed(5463436)

for (N in values_N) {
  for (alp in values_Alpha) {

    simulated_values1 <- do.call(paste0("r", fit1$family[1]), c(list(N), fit1[fit1$parameters]))
    simulated_values2 <- do.call(paste0("r", fit2$family[1]), c(list(N), fit2[fit2$parameters]))
    simulated_values_dist <- matrix(c(simulated_values1, simulated_values2), nrow = N, ncol = 2)
    
    simulated_values_copula <- rCopula(N, get(names(AIC)[which.min(AIC)])@copula)
    
    simulated_values1 <- do.call(paste0("q", fit1$family[1]), c(list(simulated_values_copula[, 1]), fit1[fit1$parameters]))
    simulated_values2 <- do.call(paste0("q", fit2$family[1]), c(list(simulated_values_copula[, 2]), fit2[fit2$parameters]))
    simulated_values_copula_changed <- data.frame(x1 = simulated_values1, x2 = simulated_values2)

    result_dist <- optimize(compute_var, interval = c(0, 1), x = simulated_values_dist, alpha = alp)
    cat("Wartości VaR (z funkcji optimize) z danych empirycznych dla N =", N, "i Alpha =", alp, 
        "wynosi", round(result_dist$objective,6))
    print_empty_line()
    
    var_results1 <- data.frame(Beta = numeric(length(betas)), VaR = numeric(length(betas)))

    for (i in seq_along(betas)) {
      var_results1[i, ] <- c(betas[i], compute_var(betas[i], x = simulated_values_dist, alp))
    }
    sorted_var_results1 <- var_results1[order(var_results1$VaR), ]
    
    cat("Wartości VaR (liczona ręcznie) z danych empirycznych dla N =", N, "i Alpha =", alp, "wynosi", round(sorted_var_results1$VaR[1],6), "\n")
    
    result_copula <- optimize(compute_var, interval = c(0, 1), x = simulated_values_copula_changed, alpha = alp)
    cat("Wartości VaR (z funkcji optimize) z danych z kopuły dla N =", N, "i Alpha =", alp, 
        "wynosi", round(result_copula$objective,6))
    print_empty_line()

    var_results2 <- data.frame(Beta = numeric(length(betas)) ,VaR = numeric(length(betas)))

    for (i in seq_along(betas)) {
      var_results2[i, ] <- c(betas[i], compute_var(betas[i], x = simulated_values_copula_changed, alp))
    }
    
    sorted_var_results2 <- var_results2[order(var_results2$VaR), ]

    cat("Wartości VaR (liczona ręcznie) z danych z kopuły dla N =", N, "i Alpha =", alp, "wynosi", round(sorted_var_results2$VaR[1],6), "\n")
    print_empty_line()
    
  }
}
```
